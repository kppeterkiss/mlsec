
@INPROCEEDINGS{MitigatingDataPoisoningAttack2021,
  author={Doku, Ronald and Rawat, Danda B.},
  booktitle={2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC)}, 
  title={Mitigating Data Poisoning Attacks On a Federated Learning-Edge Computing Network}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Support vector machines;Data privacy;Collaborative work;Data models;Numerical models;Servers},
  doi={10.1109/CCNC49032.2021.9369581}}


@article{bouacida2021vulnerabilities,
  title={Vulnerabilities in federated learning},
  author={Bouacida, Nader and Mohapatra, Prasant},
  journal={IEEe Access},
  volume={9},
  pages={63229--63249},
  year={2021},
  publisher={IEEE}
}

@article{xia2023poisoning,
  title={Poisoning attacks in federated learning: A survey},
  author={Xia, Geming and Chen, Jian and Yu, Chaodong and Ma, Jun},
  journal={Ieee Access},
  volume={11},
  pages={10708--10722},
  year={2023},
  publisher={IEEE}
}

@article{biggio2012poisoning,
  title={Poisoning attacks against support vector machines},
  author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  journal={arXiv preprint arXiv:1206.6389},
  year={2012}
}

@article{li2020preserving,
  title={Preserving data privacy via federated learning: Challenges and solutions},
  author={Li, Zengpeng and Sharma, Vishal and Mohanty, Saraju P},
  journal={IEEE Consumer Electronics Magazine},
  volume={9},
  number={3},
  pages={8--16},
  year={2020},
  publisher={IEEE}
}

@inproceedings{yin2018byzantine,
  title={Byzantine-robust distributed learning: Towards optimal statistical rates},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International conference on machine learning},
  pages={5650--5659},
  year={2018},
  organization={Pmlr}
}

@article{blanchard2017machine,
  title={Machine learning with adversaries: Byzantine tolerant gradient descent},
  author={Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}
}

@inproceedings{zenke2017continual,
  title={Continual learning through synaptic intelligence},
  author={Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={3987--3995},
  year={2017},
  organization={PMLR}
}

@article{williams1992fast,
  title={A fast algorithm for active contours and curvature estimation},
  author={Williams, Donna J and Shah, Mubarak},
  journal={CVGIP: Image understanding},
  volume={55},
  number={1},
  pages={14--26},
  year={1992},
  publisher={Elsevier}
}


@inproceedings{aljundi2019task,
  title={Task-free continual learning},
  author={Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11254--11263},
  year={2019}
}
@inproceedings{li2019learn,
  title={Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting},
  author={Li, Xilai and Zhou, Yingbo and Wu, Tianfu and Socher, Richard and Xiong, Caiming},
  booktitle={International conference on machine learning},
  pages={3925--3934},
  year={2019},
  organization={PMLR}
}

@article{lee2017overcoming,
  title={Overcoming catastrophic forgetting by incremental moment matching},
  author={Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{shmelkov2017incremental,
  title={Incremental learning of object detectors without catastrophic forgetting},
  author={Shmelkov, Konstantin and Schmid, Cordelia and Alahari, Karteek},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3400--3409},
  year={2017}
}

@inproceedings{bhagoji2019analyzing,
  title={Analyzing federated learning through an adversarial lens},
  author={Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  booktitle={International conference on machine learning},
  pages={634--643},
  year={2019},
  organization={PMLR}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}


@inproceedings{wang2019beyond,
  title={Beyond inferring class representatives: User-level privacy leakage from federated learning},
  author={Wang, Zhibo and Song, Mengkai and Zhang, Zhifei and Song, Yang and Wang, Qian and Qi, Hairong},
  booktitle={IEEE INFOCOM 2019-IEEE conference on computer communications},
  pages={2512--2520},
  year={2019},
  organization={IEEE}
}

@article{fung2018mitigating,
  title={Mitigating sybils in federated learning poisoning},
  author={Fung, Clement and Yoon, Chris JM and Beschastnikh, Ivan},
  journal={arXiv preprint arXiv:1808.04866},
  year={2018}
}

@article{chen2020training,
  title={A training-integrity privacy-preserving federated learning scheme with trusted execution environment},
  author={Chen, Yu and Luo, Fang and Li, Tong and Xiang, Tao and Liu, Zheli and Li, Jin},
  journal={Information Sciences},
  volume={522},
  pages={69--79},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{nguyen2020poisoning,
  title={Poisoning attacks on federated learning-based IoT intrusion detection system},
  author={Nguyen, Thien Duc and Rieger, Phillip and Miettinen, Markus and Sadeghi, Ahmad-Reza and others},
  booktitle={Proc. workshop decentralized IoT syst. secur.(DISS)},
  volume={79},
  pages={1--7},
  year={2020}
}

@article{li2022learning,
  title={Learning to attack federated learning: A model-based reinforcement learning attack framework},
  author={Li, Henger and Sun, Xiaolin and Zheng, Zizhan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35007--35020},
  year={2022}
}

@inproceedings{hossain2021desmp,
  title={Desmp: Differential privacy-exploited stealthy model poisoning attacks in federated learning},
  author={Hossain, Md Tamjid and Islam, Shafkat and Badsha, Shahriar and Shen, Haoting},
  booktitle={2021 17th International Conference on Mobility, Sensing and Networking (MSN)},
  pages={167--174},
  year={2021},
  organization={IEEE}
}

@inproceedings{shejwalkar2022back,
  title={Back to the drawing board: A critical evaluation of poisoning attacks on production federated learning},
  author={Shejwalkar, Virat and Houmansadr, Amir and Kairouz, Peter and Ramage, Daniel},
  booktitle={2022 IEEE symposium on security and privacy (SP)},
  pages={1354--1371},
  year={2022},
  organization={IEEE}
}

@article{sun2021data,
  title={Data poisoning attacks on federated machine learning},
  author={Sun, Gan and Cong, Yang and Dong, Jiahua and Wang, Qiang and Lyu, Lingjuan and Liu, Ji},
  journal={IEEE Internet of Things Journal},
  volume={9},
  number={13},
  pages={11365--11375},
  year={2021},
  publisher={IEEE}
}

@article{sun2019can,
  title={Can you really backdoor federated learning?},
  author={Sun, Ziteng and Kairouz, Peter and Suresh, Ananda Theertha and McMahan, H Brendan},
  journal={arXiv preprint arXiv:1911.07963},
  year={2019}
}

@inproceedings{xie2019dba,
  title={Dba: Distributed backdoor attacks against federated learning},
  author={Xie, Chulin and Huang, Keli and Chen, Pin-Yu and Li, Bo},
  booktitle={International conference on learning representations},
  year={2019}
}

@article{zhou2021deep,
  title={Deep model poisoning attack on federated learning},
  author={Zhou, Xingchen and Xu, Ming and Wu, Yiming and Zheng, Ning},
  journal={Future Internet},
  volume={13},
  number={3},
  pages={73},
  year={2021},
  publisher={MDPI}
}

@article{zhang2020poisongan,
  title={PoisonGAN: Generative poisoning attacks against federated learning in edge computing systems},
  author={Zhang, Jiale and Chen, Bing and Cheng, Xiang and Binh, Huynh Thi Thanh and Yu, Shui},
  journal={IEEE Internet of Things Journal},
  volume={8},
  number={5},
  pages={3310--3322},
  year={2020},
  publisher={IEEE}
}

@inproceedings{cao2022mpaf,
  title={Mpaf: Model poisoning attacks to federated learning based on fake clients},
  author={Cao, Xiaoyu and Gong, Neil Zhenqiang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3396--3404},
  year={2022}
}

@inproceedings{zhao2019pdgan,
  title={PDGAN: A novel poisoning defense method in federated learning using generative adversarial network},
  author={Zhao, Ying and Chen, Junjun and Zhang, Jiale and Wu, Di and Teng, Jian and Yu, Shui},
  booktitle={International conference on algorithms and architectures for parallel processing},
  pages={595--609},
  year={2019},
  organization={Springer}
}

@article{konevcny2016federated,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1610.02527},
  year={2016}
}
%%fedavg -duplicate
@article{mcmahan2016communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and others},
  journal={arXiv preprint arXiv:1602.05629},
  year={2016}
}
@article{kairouz2019advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}
@inproceedings{yao1986generate,
  title={How to generate and exchange secrets},
  author={Yao, Andrew Chi-Chih},
  booktitle={27th Annual Symposium on Foundations of Computer Science (sfcs 1986)},
  pages={162--167},
  year={1986},
  organization={IEEE}
}
@inproceedings{gentry2009fully,
  title={Fully homomorphic encryption using ideal lattices},
  author={Gentry, Craig},
  booktitle={Proceedings of the forty-first annual ACM symposium on Theory of computing},
  pages={169--178},
  year={2009}
}
@inproceedings{ishai2003extending,
  title={Extending oblivious transfers efficiently},
  author={Ishai, Yuval and Kilian, Joe and Nissim, Kobbi and Petrank, Erez},
  booktitle={Annual International Cryptology Conference},
  pages={145--161},
  year={2003},
  organization={Springer}
}
@inproceedings{subramanyan2017formal,
  title={A formal foundation for secure remote execution of enclaves},
  author={Subramanyan, Pramod and Sinha, Rohit and Lebedev, Ilia and Devadas, Srinivas and Seshia, Sanjit A},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={2435--2450},
  year={2017}
}
@inproceedings{bittau2017prochlo,
  title={Prochlo: Strong privacy for analytics in the crowd},
  author={Bittau, Andrea and Erlingsson, {\'U}lfar and Maniatis, Petros and Mironov, Ilya and Raghunathan, Ananth and Lie, David and Rudominer, Mitch and Kode, Ushasree and Tinnes, Julien and Seefeld, Bernhard},
  booktitle={Proceedings of the 26th Symposium on Operating Systems Principles},
  pages={441--459},
  year={2017}
}
@article{chaum1981untraceable,
  title={Untraceable electronic mail, return addresses, and digital pseudonyms},
  author={Chaum, David L},
  journal={Communications of the ACM},
  volume={24},
  number={2},
  pages={84--90},
  year={1981},
  publisher={ACM New York, NY, USA}
}
@article{kwon2016riffle,
  title={Riffle: An efficient communication system with strong anonymity},
  author={Kwon, Albert and Lazar, David and Devadas, Srinivas and Ford, Bryan},
  journal={Proceedings on Privacy Enhancing Technologies},
  volume={2016},
  number={2},
  pages={115--134},
  year={2016},
  publisher={Sciendo}
}
@article{reyzin2018turning,
  title={Turning HATE Into LOVE: Homomorphic Ad Hoc Threshold Encryption for Scalable MPC.},
  author={Reyzin, Leonid and Smith, Adam D and Yakoubov, Sophia},
  journal={IACR Cryptol. ePrint Arch.},
  volume={2018},
  pages={997},
  year={2018}
}
@inproceedings{roth2019honeycrisp,
  title={Honeycrisp: large-scale differentially private aggregation without a trusted core},
  author={Roth, Edo and Noble, Daniel and Falk, Brett Hemenway and Haeberlen, Andreas},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={196--210},
  year={2019}
}


@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}
@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages={308--318},
  year={2016},
  organization={ACM}
 }
@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}


@inproceedings{Privacy_preserving_DL,
 author = {Shokri, Reza and Shmatikov, Vitaly},
 title = {Privacy-Preserving Deep Learning},
 booktitle = {Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security},
 series = {CCS '15},
 year = {2015},
 isbn = {978-1-4503-3832-5},
 location = {Denver, Colorado, USA},
 pages = {1310--1321},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2810103.2813687},
 doi = {10.1145/2810103.2813687},
 acmid = {2813687},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deep learning, gradient descent, neural networks, privacy},
} 

@InProceedings{diff_priv2,
author="Dwork, Cynthia
and McSherry, Frank
and Nissim, Kobbi
and Smith, Adam",
editor="Halevi, Shai
and Rabin, Tal",
title="Calibrating Noise to Sensitivity in Private Data Analysis",
booktitle="Theory of Cryptography",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="265--284",
abstract="We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.",
isbn="978-3-540-32732-5"
}

@inproceedings{fredrikson2015model,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
  pages={1322--1333},
  year={2015}
}



@inproceedings{ShokriSS16membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}



@article{shamir1979share,
  title={How to share a secret},
  author={Shamir, Adi},
  journal={Communications of the ACM},
  volume={22},
  number={11},
  pages={612--613},
  year={1979},
  publisher={ACm New York, NY, USA}
}

@article{diffie1976new,
  title={New directions in cryptography},
  author={Diffie, Whitfield and Hellman, Martin},
  journal={IEEE transactions on Information Theory},
  volume={22},
  number={6},
  pages={644--654},
  year={1976},
  publisher={IEEE}
}


@article{humphries2021differentially,
  title={Differentially Private Learning Does Not Bound Membership Inference},
  author={Humphries, Thomas and Rafuse, Matthew and Tulloch, Lindsey and Oya, Simon and Goldberg, Ian and Hengartner, Urs and Kerschbaum, Florian},
  journal={arXiv preprint arXiv:2010.12112},
  year={2020}
}
@inproceedings{shi2011privacy,
  title={Privacy-preserving aggregation of time-series data},
  author={Shi, Elaine and Chan, TH Hubert and Rieffel, Eleanor and Chow, Richard and Song, Dawn},
  booktitle={Proc. NDSS},
  volume={2},
  pages={1--17},
  year={2011},
  organization={Citeseer}
}
@inproceedings{halevi2011secure,
  title={Secure computation on the web: Computing without simultaneous interaction},
  author={Halevi, Shai and Lindell, Yehuda and Pinkas, Benny},
  booktitle={Annual Cryptology Conference},
  pages={132--150},
  year={2011},
  organization={Springer}
}
@inproceedings{segal2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}

@article{geiping2020inverting,
  title={Inverting Gradients--How easy is it to break privacy in federated learning?},
  author={Geiping, Jonas and Bauermeister, Hartmut and Dr{\"o}ge, Hannah and Moeller, Michael},
  journal={arXiv preprint arXiv:2003.14053},
  year={2020}
}
@inproceedings{aono2017privacy1,
  title={Privacy-preserving deep learning: Revisited and enhanced},
  author={Aono, Yoshinori and Hayashi, Takuya and Wang, Lihua and Moriai, Shiho and others},
  booktitle={International Conference on Applications and Techniques in Information Security},
  pages={100--110},
  year={2017},
  organization={Springer}
}

@inproceedings{hitaj2017deep,
  title={Deep models under the GAN: information leakage from collaborative deep learning},
  author={Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={603--618},
  year={2017}
}

@inproceedings{szegedy2014going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@article{goodfellow2015explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{carlini2018audio,
  title={Audio adversarial examples: Targeted attacks on speech-to-text},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={1--7},
  year={2018},
  organization={IEEE}
}

@inproceedings{carlini2016hidden,
  title={Hidden voice commands},
  author={Carlini, Nicholas and Mishra, Pratyush and Vaidya, Tavish and Zhang, Yuankai and Sherr, Micah and Shields, Clay and Wagner, David and Zhou, Wenchao},
  booktitle={25th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 16)},
  pages={513--530},
  year={2016}
}

@inproceedings{ilyas2018blackbox,
  title={Black-box adversarial attacks with limited queries and information},
  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
  booktitle={International Conference on Machine Learning},
  pages={2137--2146},
  year={2018},
  organization={PMLR}
}

@article{gu2019badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}

@inproceedings{bagdasaryan2020backdoor,
  title={How to backdoor federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2938--2948},
  year={2020},
  organization={PMLR}
}

@misc{chen2017targeted,
      title={Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning}, 
      author={Xinyun Chen and Chang Liu and Bo Li and Kimberly Lu and Dawn Song},
      year={2017},
      eprint={1712.05526},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{Biggio_2013,
   title={Evasion Attacks against Machine Learning at Test Time},
   ISBN={9783642387098},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-642-40994-3_25},
   DOI={10.1007/978-3-642-40994-3_25},
   journal={Lecture Notes in Computer Science},
   publisher={Springer Berlin Heidelberg},
   author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Šrndić, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
   year={2013},
   pages={387–402}
}
@article{finlayson2018adversarial,
  title={Adversarial attacks against medical deep learning systems},
  author={Finlayson, Samuel G and Chung, Hyung Won and Kohane, Isaac S and Beam, Andrew L},
  journal={arXiv preprint arXiv:1804.05296},
  year={2018}
}
@article{aono2017privacy,
  title={Privacy-preserving deep learning via additively homomorphic encryption},
  author={Aono, Yoshinori and Hayashi, Takuya and Wang, Lihua and Moriai, Shiho and others},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={13},
  number={5},
  pages={1333--1345},
  year={2017},
  publisher={IEEE}
}
@article{zhao2020idlg,
  title={idlg: Improved deep leakage from gradients},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  journal={arXiv preprint arXiv:2001.02610},
  year={2020}
}

@inproceedings{biggio_poisoning_svm_2012,
author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
title = {Poisoning attacks against support vector machines},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data.The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1467–1474},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

  

@article{zhu2019deep,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{ribeiro2020accuracy,
  title={Beyond accuracy: Behavioral testing of NLP models with CheckList},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  journal={arXiv preprint arXiv:2005.04118},
  year={2020}
}
@article{rudin1992nonlinear,
  title={Nonlinear total variation based noise removal algorithms},
  author={Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
  journal={Physica D: nonlinear phenomena},
  volume={60},
  number={1-4},
  pages={259--268},
  year={1992},
  publisher={Elsevier}
}
@article{jayaraman2021revisiting,
  title={Revisiting membership inference under realistic assumptions},
  author={Jayaraman, Bargav and Wang, Lingxiao and Knipmeyer, Katherine and Gu, Quanquan and Evans, David},
  journal={arXiv preprint arXiv:2005.10881},
  year={2020}
}
@article{long2017measuring,
  title={Towards measuring membership privacy},
  author={Long, Yunhui and Bindschaedler, Vincent and Gunter, Carl A},
  journal={arXiv preprint arXiv:1712.09136},
  year={2017}
}

@article{salem2018mlleaks,
  title={Ml-leaks: Model and data independent membership inference attacks and defenses on machine learning models},
  author={Salem, Ahmed and Zhang, Yang and Humbert, Mathias and Berrang, Pascal and Fritz, Mario and Backes, Michael},
  journal={arXiv preprint arXiv:1806.01246},
  year={2018}
}

@inproceedings{yeom2018privacy,
  title={Privacy risk in machine learning: Analyzing the connection to overfitting},
  author={Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
  booktitle={2018 IEEE 31st Computer Security Foundations Symposium (CSF)},
  pages={268--282},
  year={2018},
  organization={IEEE}
}

@inproceedings{fredrikson2014privacy,
  title={Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing},
  author={Fredrikson, Matthew and Lantz, Eric and Jha, Somesh and Lin, Simon and Page, David and Ristenpart, Thomas},
  booktitle={23rd $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 14)},
  pages={17--32},
  year={2014}
}

@article{ateniese2013hacking,
  title={Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers},
  author={Ateniese, Giuseppe and Mancini, Luigi V and Spognardi, Angelo and Villani, Antonio and Vitali, Domenico and Felici, Giovanni},
  journal={International Journal of Security and Networks},
  volume={10},
  number={3},
  pages={137--150},
  year={2015},
  publisher={Inderscience Publishers (IEL)}
}

@inproceedings{ganju2018property,
  title={Property inference attacks on fully connected neural networks using permutation invariant representations},
  author={Ganju, Karan and Wang, Qi and Yang, Wei and Gunter, Carl A and Borisov, Nikita},
  booktitle={Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
  pages={619--633},
  year={2018}
}
@inproceedings{lowd2005adversarial,
  title={Adversarial learning},
  author={Lowd, Daniel and Meek, Christopher},
  booktitle={Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining},
  pages={641--647},
  year={2005}
}

@inproceedings{wang2019stealing,
  title={Stealing hyperparameters in machine learning},
  author={Wang, Binghui and Gong, Neil Zhenqiang},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)},
  pages={36--52},
  year={2018},
  organization={IEEE}
}
@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction apis},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 16)},
  pages={601--618},
  year={2016}
}

@inproceedings{yan2018cache,
  title={Cache telepathy: Leveraging shared resource attacks to learn $\{$DNN$\}$ architectures},
  author={Yan, Mengjia and Fletcher, Christopher W and Torrellas, Josep},
  booktitle={29th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 20)},
  pages={2003--2020},
  year={2020}
}
@misc{shokri2017membership,
      title={Membership Inference Attacks against Machine Learning Models}, 
      author={Reza Shokri and Marco Stronati and Congzheng Song and Vitaly Shmatikov},
      year={2017},
      eprint={1610.05820},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@inproceedings{jayaraman2019evaluating,
  title={Evaluating differentially private machine learning in practice},
  author={Jayaraman, Bargav and Evans, David},
  booktitle={28th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 19)},
  pages={1895--1912},
  year={2019}
}
@article{bernau2020assessing,
  title={Assessing differentially private deep learning with membership inference},
  author={Bernau, Daniel and Grassal, Philip-William and Robl, Jonas and Kerschbaum, Florian},
  journal={arXiv preprint arXiv:1912.11328},
  year={2019}
}

@inproceedings{liu2016dependence,
  title={Dependence Makes You Vulnberable: Differential Privacy Under Dependent Tuples.},
  author={Liu, Changchang and Chakraborty, Supriyo and Mittal, Prateek},
  booktitle={NDSS},
  volume={16},
  pages={21--24},
  year={2016}
}

@article{almadhoun2020inference,
  title={Inference attacks against differentially private query results from genomic datasets including dependent tuples},
  author={Almadhoun, Nour and Ayday, Erman and Ulusoy, {\"O}zg{\"u}r},
  journal={Bioinformatics},
  volume={36},
  number={Supplement\_1},
  pages={i136--i145},
  year={2020},
  publisher={Oxford University Press}
}
@inproceedings{tschantz2020sok,
  title={SoK: Differential Privacy as a Causal Property},
  author={Tschantz, Michael Carl and Sen, Shayak and Datta, Anupam},
  booktitle={2020 IEEE Symposium on Security and Privacy (SP)},
  pages={354--371},
  year={2020},
  organization={IEEE}
}

@inproceedings{wu2016methodology,
  title={A methodology for formalizing model-inversion attacks},
  author={Wu, Xi and Fredrikson, Matthew and Jha, Somesh and Naughton, Jeffrey F},
  booktitle={2016 IEEE 29th Computer Security Foundations Symposium (CSF)},
  pages={355--370},
  year={2016},
  organization={IEEE}
}
@article{zhao2025federation,
  title={The Federation Strikes Back: A Survey of Federated Learning Privacy Attacks, Defenses, Applications, and Policy Landscape},
  author={Zhao, Joshua and Bagchi, Saurabh and Avestimehr, Salman and Chan, Kevin and Chaterji, Somali and Dimitriadis, Dimitris and Li, Jiacheng and Li, Ninghui and Nourian, Arash and Roth, Holger},
  journal={ACM Computing Surveys},
  volume={57},
  number={9},
  pages={1--37},
  year={2025},
  publisher={ACM New York, NY}
}
@misc{fowl2022robbingfeddirectlyobtaining,
      title={Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models}, 
      author={Liam Fowl and Jonas Geiping and Wojtek Czaja and Micah Goldblum and Tom Goldstein},
      year={2022},
      eprint={2110.13057},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.13057}, 
}
@inproceedings{boenisch2023curious,
  title={When the curious abandon honesty: Federated learning is not private},
  author={Boenisch, Franziska and Dziedzic, Adam and Schuster, Roei and Shamsabadi, Ali Shahin and Shumailov, Ilia and Papernot, Nicolas},
  booktitle={2023 IEEE 8th European Symposium on Security and Privacy (EuroS\&P)},
  pages={175--199},
  year={2023},
  organization={IEEE}
}
@inproceedings{zhao2024loki,
  title={Loki: Large-scale data reconstruction attack against federated learning through model manipulation},
  author={Zhao, Joshua C and Sharma, Atul and Elkordy, Ahmed Roushdy and Ezzeldin, Yahya H and Avestimehr, Salman and Bagchi, Saurabh},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
  pages={1287--1305},
  year={2024},
  organization={IEEE}
}
@inproceedings{yin2021see,
  title={See through gradients: Image batch recovery via gradinversion},
  author={Yin, Hongxu and Mallya, Arun and Vahdat, Arash and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16337--16346},
  year={2021}
}

@misc{eu_ai_act2023,
    author={{European Parliament}},      
    title={{EU AI Act: First regulation on artificial intelligence}}, 
    year={2023},
    url={https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence}
}

@article{kotyan_adv_attacks_2023,
  title={A reading survey on adversarial machine learning: Adversarial attacks and their under-standing},
  author={S. Kotyan},
  journal={arXiv preprint arXiv:2308.03363},
  year={2023}
}

@article{zadeh_fuzzy_1965,
  title={Fuzzy sets},
  author={L. A. Zadeh},
  journal={Information and control},
  year={1965},
  volume={8},
  number={3},
  pages={338-353}
}

@misc{linnainmaa_backprop_1970,
  title={The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors},
  author={S. Linnainmaa},
  journal={Masters dissertation (in Finnish)},
  publisher={University of Helsinki (Finland)},
  year={1970},
  pages={6-7}
}

@misc{owasp_ml_sec_2023,
  title={{OWASP Machine Learning Security Top Ten}},
  author={{OWASP}},
  year={2023},
  url={https://owasp.org/www-project-machine-learning-security-top-10/}
}

@misc{owasp_llm_sec_2025,
  title={{OWASP Top 10 for LLM Applications 2025}},
  author={{OWASP}},
  year={2025},
  url={https://genai.owasp.org/download/43299/?tmstv=1731900559}
}

@misc{WWT_prompt_injection_lab,
  title={{AI Prompt Injection Lab}},
  author={{World Wide Technology (WWT)}},
  year={2026},
  url={https://www.wwt.com/lab/ai-prompt-injection-lab}
}


@misc{nist_adv_ml_2024,
  title={{Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations}},
  author={{NIST}},
  year={2024},
  url={https://doi.org/10.6028/NIST.AI.100-2e2023}
}

@book{ackerman_ics_security_2017,
    author = {Pascal Ackerman},
    title = {{Industrial Cybersecurity – Efficiently secure critical infrastructure systems}},
    publisher = {Packt Publishing},
    year = 2017
}

@book{bodungen_hacking_ics_2016,
    author = {C. Bodungen and B. Singer and A. Shbeeb and K. Wilhoit and S. Hilt},
    title = {{Hacking Exposed – Industrial Control Systems: ICS and SCADA Security Secrets \& Solutions}},
    publisher = {{McGraw-Hill Education}},
    year = 2016
}

@book{fennely_physical_security_2013,
    author = {L.J. Fennelly},
    title = {Effective physical security},
    publisher = {Butterworth-Heinemann},
    year = 2013
}

@article{tiszolczi_fizikai_iso27001_2019,
    author = {B.G. Tiszolczi},
    title = {{Fizikai biztons{\'a}gi kontrollok tervez{\'e}s{\'e}nek és alkalmaz{\'a}s{\'a}nak gyakorlata az ISO/IEC 27001 szabv{\'a}ny elv{\'a}r{\'a}sainak t{\''u}kr{\'e}ben}},
    journal = {Magyar Rend{\'e}szet},
    year = 2019,
    volume = {2019/2—3},
    pages = {233—249},
    url={10.32577/mr.2019.2-3.12}
}

@misc{uptime_inst_physical_sec_levels,
  title={{Tier Classification System}},
  author={{Uptime Institute}},
  year={2025},
  url={https://uptimeinstitute.com/tiers}
}

@misc{metcalf_sniper_attack_2013,
  title={{Metcalf sniper attack}},
  author={Wikipedia},
  year = 2013,
  url={https://en.wikipedia.org/wiki/Metcalf_sniper_attack}
}

@article{yin_transferability_2023,
title = {Transfer adversarial attacks across industrial intelligent systems},
journal = {Reliability Engineering \& System Safety},
volume = {237},
pages = {109299},
year = {2023},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109299},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023002132},
author = {Zhenqin Yin and Yue Zhuo and Zhiqiang Ge},
keywords = {Intelligent systems security, Industrial intelligent systems, Adversarial attack, Transfer-based attack, Adversarial defense},
abstract = {As indispensable parts of industrial production control, data-driven industrial intelligent systems (IIS) achieve efficient executions of significant tasks such as fault classification (FC), fault detection (FD), and soft sensing (SS). Recently, machine learning models have been proven vulnerable to adversarial attacks, where the transfer-based attacks provide highly feasible attacks on systems in real-world black-box scenarios. In this paper, to study the practical security risks of IIS, we investigate transferable adversarial attacks from: (1) showing the existence of transferable adversarial examples across different industrial tasks; (2) exploring factors (e.g., data feature, model structure, and attack method) affecting transferability under multi-scenarios; (3) proposing a new method to enhance the transferability; (4) providing guidelines on practical system deployments to defend against transferable adversarial threats. The attacks demonstrate generality on two types of datasets, Tennessee Eastman industrial process (TEP) and WM-811K wafer map dataset, and the experiment results show that: (1) transfer is asymmetric and complex models are relatively stable with low sample transferability; (2) iterative and single-step methods have opposite performance characteristics under the intra- and cross-task transfer; (3) overfitting of optimization methods leads to weak transferability; (4) smoothing gradients and widening intermediate layer perturbations are effective directions for improving transferability.}
}

@misc{solarwinds_attack_2020,
  title={{2020 United States federal government data breach (Solarwinds)}},
  author={Wikipedia},
  year = 2020,
  url={https://en.wikipedia.org/wiki/2020_United_States_federal_government_data_breach}
}

@book{bochman_cyber_informed_2021,
    author = {Andrew A. Bochman and Sarah Freeman},
    title = {Countering Cyber Sabotage: Introducing Consequence-Driven, Cyber-Informed Engineering (CCE)},
    publisher = {CRC Press; 1st Edition},
    year = 2021
}

@misc{usa_privacy_act1974,
    author={{United States of America (USA)}},      
    title={{Privacy Act}}, 
    year={1974},
    url={https://www.justice.gov/sites/default/files/opcl/docs/1974privacyact-2012.pdf}
}

@misc{eu_gdpr_2018,
    author={{European Parliament and European Commission}},      
    title={{General Data Protection Regulation}}, 
    year={2016},
    url={https://eur-lex.europa.eu/eli/reg/2016/679/oj/eng}
}

@misc{eu_nis2_2022,
    author={{European Parliament and European Commission}},      
    title={{NIS 2 Directive}}, 
    year={2022},
    url={https://eur-lex.europa.eu/eli/dir/2022/2555/2022-12-27/eng},
    abstract = { Directive (EU) 2022/2555 of the European Parliament and of the Council of 14 December 2022 on measures for a high common level of cybersecurity across the Union, amending Regulation (EU) No 910/2014 and Directive (EU) 2018/1972, and repealing Directive (EU) 2016/1148 (NIS 2 Directive) }
}

@inproceedings{dalvi_adversarial_classification_2004,
  title={Adversarial classification},
  author={Dalvi, Nilesh and Domingos, Pedro and Mausam and Sanghai, Sumit and Verma, Deepak},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={99--108},
  year={2004},
  organization={Association for Computing Machinery (ACM)},
    doi={https://doi.org/10.1145/1014052.1014066}
}

@article{huang_sponge_poision_2024,
  author={Huang, Benxuan and Pang, Lihui and Fu, Anmin and Al-Sarawi, Said F. and Abbott, Derek and Gao, Yansong},
  journal={IEEE Access}, 
  title={Sponge Attack Against Multi-Exit Networks With Data Poisoning}, 
  year={2024},
  volume={12},
  number={},
  pages={33843-33851},
  keywords={Training;Data models;Computational modeling;Perturbation methods;Object detection;Optimization;Meteorology;Data integrity;Machine learning;Computer network management;Data poisoning;sponge attack;multi-exit network;machine learning},
  doi={10.1109/ACCESS.2024.3370849}
}

@INPROCEEDINGS{boucher_adversarial_perturbations_2022,
  author={Boucher, Nicholas and Shumailov, Ilia and Anderson, Ross and Papernot, Nicolas},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={Bad Characters: Imperceptible NLP Attacks}, 
  year={2022},
  volume={},
  number={},
  pages={1987-2004},
  keywords={Visualization;Toxicology;Systematics;Social networking (online);Perturbation methods;Taxonomy;Natural language processing;adversarial machine learning;NLP;text-based models;text encodings;search engines},
  doi={10.1109/SP46214.2022.9833641}
}

@misc{hasan_sponge_energy_latency2025,
      title={Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning}, 
      author={Syed Mhamudul Hasan and Hussein Zangoti and Iraklis Anagnostopoulos and Abdur R. Shahid},
      year={2025},
      eprint={2505.06454},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.06454}, 
}

@INPROCEEDINGS{shumailov_sponge_energy_latency_2021,
  author={Shumailov, Ilia and Zhao, Yiren and Bates, Daniel and Papernot, Nicolas and Mullins, Robert and Anderson, Ross},
  booktitle={2021 IEEE European Symposium on Security and Privacy (EuroS\&P)}, 
  title={Sponge Examples: Energy-Latency Attacks on Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={212-231},
  keywords={Deep learning;Training;Performance evaluation;Knowledge engineering;Energy consumption;Neural networks;Graphics processing units;availability attacks;adversarial machine learning;adversarial examples;sponge examples;latency attacks;denial of service},
  doi={10.1109/EuroSP51992.2021.00024}
}

@article{cina_sponge_2025,
title = {Energy-latency attacks via sponge poisoning},
journal = {Information Sciences},
volume = {702},
pages = {121905},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.121905},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525000374},
author = {Antonio Emanuele Cinà and Ambra Demontis and Battista Biggio and Fabio Roli and Marcello Pelillo},
keywords = {Energy poisoning, Sponge poisoning attacks, Adversarial machine learning, Deep neural networks, AI security},
abstract = {Sponge examples are test-time inputs optimized to increase energy consumption and prediction latency of deep networks deployed on hardware accelerators. By increasing the fraction of neurons activated during classification, these attacks reduce sparsity in network activation patterns, worsening the performance of hardware accelerators. In this work, we present a novel training-time attack, named sponge poisoning, which aims to worsen energy consumption and prediction latency of neural networks on any test input without affecting classification accuracy. To stage this attack, we assume that the attacker can control only a few model updates during training — a likely scenario, e.g., when model training is outsourced to an untrusted third party or distributed via federated learning. Our extensive experiments on image classification tasks show that sponge poisoning is effective, and that fine-tuning poisoned models to repair them poses prohibitive costs for most users, highlighting that tackling sponge poisoning remains an open issue.}
}

@misc{gao_DOS_spong_LLM_2024,
      title={Denial-of-Service Poisoning Attacks against Large Language Models}, 
      author={Kuofeng Gao and Tianyu Pang and Chao Du and Yong Yang and Shu-Tao Xia and Min Lin},
      year={2024},
      eprint={2410.10760},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2410.10760}, 
}

@INPROCEEDINGS {shapira_sponge_yolo_2023,
author = { Shapira, Avishag and Zolfi, Alon and Demetrio, Luca and Biggio, Battista and Shabtai, Asaf },
booktitle = { 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) },
title = {{ Phantom Sponges: Exploiting Non-Maximum Suppression to Attack Deep Object Detectors }},
year = {2023},
volume = {},
ISSN = {},
pages = {4560-4569},
abstract = { Adversarial attacks against deep learning-based object detectors have been studied extensively in the past few years. Most of the attacks proposed have targeted the model’s integrity (i.e., caused the model to make incorrect predictions), while adversarial attacks targeting the model’s availability, a critical aspect in safety-critical domains such as autonomous driving, have not yet been explored by the machine learning research community. In this paper, we propose a novel attack that negatively affects the decision latency of an end-to-end object detection pipeline. We craft a universal adversarial perturbation (UAP) that targets a widely used technique integrated in many object detector pipelines – non-maximum suppression (NMS). Our experiments demonstrate the proposed UAP’s ability to increase the processing time of individual frames by adding "phantom" objects that overload the NMS algorithm while preserving the detection of the original objects which allows the attack to go undetected for a longer period of time. },
keywords = {Perturbation methods;Pipelines;Phantoms;Detectors;Object detection;Predictive models;Prediction algorithms},
doi = {10.1109/WACV56688.2023.00455},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV56688.2023.00455},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jan
}

@article{Martin_user_behavior_2021,
  title={A survey for user behavior analysis based on machine learning techniques: current models and applications},
  author={G. Mart{\'\i}n, Alejandro and Fern{\'a}ndez-Isabel, Alberto and Mart{\'\i}n de Diego, Isaac and Beltr{\'a}n, Marta},
  journal={Applied Intelligence},
  volume={51},
  number={8},
  pages={6029--6055},
  year={2021},
  publisher={Springer}
}

@article{becker_llm_confidence_2024,
  title={Cycles of thought: Measuring llm confidence through stable explanations},
  author={Becker, Evan and Soatto, Stefano},
  journal={arXiv preprint arXiv:2406.03441},
  year={2024}
}

@article{regazzoni_watermarking_survey_2021,
  title={Protecting artificial intelligence IPs: a survey of watermarking and fingerprinting for machine learning},
  author={Regazzoni, Francesco and Palmieri, Paolo and Smailbegovic, Fethulah and Cammarota, Rosario and Polian, Ilia},
  journal={CAAI Transactions on Intelligence Technology},
  volume={6},
  number={2},
  pages={180--191},
  year={2021},
  publisher={Wiley Online Library}
}

@article{nagai_watermarking_dnn_2018,
  title={Digital watermarking for deep neural networks},
  author={Nagai, Yuki and Uchida, Yusuke and Sakazawa, Shigeyuki and Satoh, Shin’ichi},
  journal={International Journal of Multimedia Information Retrieval},
  volume={7},
  number={1},
  pages={3--16},
  year={2018},
  publisher={Springer}
}

@article{nie_watermarking_dataset_2024,
  title={Deep model intellectual property protection with compression-resistant model watermarking},
  author={Nie, Hewang and Lu, Songfeng and Wu, Junjun and Zhu, Jianxin},
  journal={IEEE Transactions on Artificial Intelligence},
  volume={5},
  number={7},
  pages={3362--3373},
  year={2024},
  publisher={IEEE}
}

@inproceedings{kirchenbauer_watermark_output_2023,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17061--17084},
  year={2023},
  organization={PMLR}
}

@article{andriushchenko_adversarial_training_2020,
  title={Understanding and improving fast adversarial training},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16048--16059},
  year={2020}
}

@misc{catak_adversarial_learning_on_medium_2020,
      title={Adversarial Machine Learning Mitigation: Adversarial Learning}, 
      author={Ferhat Ozgur Catak},
      year={2020},
      url={https://medium.com/data-science/adversarial-machine-learning-mitigation-adversarial-learning-9ae04133c137}, 
}

@misc{turing_synth_data_tools_2022,
      title={Synthetic Data Generation: Definition, Types, Techniques, and Tools}, 
      author={Turing.com},
      year={2022},
      url={https://www.turing.com/kb/synthetic-data-generation-techniques}, 
}

@incollection{polikar_ensemble_2012,
  title={Ensemble learning},
  author={Polikar, Robi},
  booktitle={Ensemble machine learning},
  pages={1--34},
  year={2012},
  publisher={Springer}
}

@article{minh_explainable_survey_2022,
  title={Explainable artificial intelligence: a comprehensive review},
  author={Minh, Dang and Wang, H Xiang and Li, Y Fen and Nguyen, Tan N},
  journal={Artificial Intelligence Review},
  volume={55},
  number={5},
  pages={3503--3568},
  year={2022},
  publisher={Springer}
}

@article{lundberg_explainability_shap_2017,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{ribeiro_explainability_lime_2016,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{simonyan_explainability_saliency_maps_2013,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}

@book{lakshmi_explainable_sustainability_2024,
  title={Explainable AI (XAI) for sustainable development: Trends and applications},
  author={Lakshmi, D and Tiwari, Ravi Shekhar and Dhanaraj, Rajesh Kumar and Kadry, Seifedine},
  year={2024},
  publisher={CRC Press}
}

@inproceedings{shan_nightshade_gai_poisoning_2024,
  title={Nightshade: Prompt-specific poisoning attacks on text-to-image generative models},
  author={Shan, Shawn and Ding, Wenxin and Passananti, Josephine and Wu, Stanley and Zheng, Haitao and Zhao, Ben Y},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
  pages={807--825},
  year={2024},
  organization={IEEE}
}

@article{guillo_on_bertillon_antropomorph_2008,
  title={Bertillon, l’anthropologie criminelle et l’histoire naturelle: des r{\'e}ponses au brouillage des identit{\'e}s},
  author={Guillo, Dominique},
  journal={Crime, Histoire \& Soci{\'e}t{\'e}s/Crime, History \& Societies},
  volume={12},
  number={1},
  pages={97--117},
  year={2008},
  publisher={Droz}
}

@book{cole_suspect_identities_2009,
  title={Suspect identities: A history of fingerprinting and criminal identification},
  author={Cole, Simon A and others},
  year={2009},
  publisher={Harvard University Press}
}

@book{song_forensics_washing_away_1981,
  title={The washing away of wrongs: forensic medicine in thirteenth-century China},
  author={Song, Ci},
  volume={1},
  year={1981},
  publisher={University of Michigan Press}
}

@book{kirk_forensics_crime_1953,
  title={Crime investigation: physical evidence and the police laboratory},
  author={Kirk, Paul Leland and Kirk, Paul L},
  year={1953},
  publisher={Interscience Publishers New York}
}

@article{michaleas_forensics_mathieu_toxicology_2022,
  title={Mathieu Joseph Bonaventure Orfila (1787-1853): The Founder of Modern Toxicology},
  author={Michaleas, Spyros N and Veskoukis, Aristidis S and Samonis, George and Pantos, Constantinos and Androutsos, Georges and Karamanou, Marianna},
  journal={Maedica},
  volume={17},
  number={2},
  pages={532},
  year={2022}
}

@book{fisher_forensics_intro_2009,
  title={Introduction to criminalistics: the foundation of forensic science},
  author={Fisher, Barry AJ and Tilstone, William J and Woytowicz, Catherine},
  year={2009},
  publisher={Academic Press}
}

@article{galton_forensics_fingerprints_1892,
  title={Finger Prints MacMillan and Co},
  author={Galton, F},
  journal={London and New York},
  year={1892}
}

@article{caplan_forensics_fingerprints_1990,
  title={How fingerprints came into use for personal identification},
  author={Caplan, Richard M},
  journal={Journal of the American Academy of Dermatology},
  volume={23},
  number={1},
  pages={109--114},
  year={1990},
  publisher={Elsevier}
}

@book{locard_forensics_criminelle_1920,
  title={L'enqu{\^e}te criminelle et les m{\'e}thodes scientifiques},
  author={Locard, Edmond},
  year={1920},
  publisher={Ernest Flammarion}
}

@article{gardner_forensics_usa_2022,
  title={Forensic science in the United States. I: Historical development and the forensic science laboratory system},
  author={Gardner, Elizabeth A and DellaRocco, Rana and Bever, Robert},
  journal={Forensic Science Review},
  volume={34},
  number={2},
  pages={72--82},
  year={2022},
  publisher={Forensic Science Review}
}

@article{jeffreys_forensics_dns_1985,
  title={Hypervariable ‘minisatellite’regions in human DNA},
  author={Jeffreys, Alec J and Wilson, Victoria and Thein, Swee Lay},
  journal={Nature},
  volume={314},
  number={6006},
  pages={67--73},
  year={1985},
  publisher={Nature Publishing Group UK London}
}

@book{casey_forensics_digital_2011,
  title={Digital evidence and computer crime: Forensic science, computers, and the internet},
  author={Casey, Eoghan},
  year={2011},
  publisher={Academic press}
}

@book{sammes_forensic_computing_2007,
  title={Forensic computing},
  author={Sammes, Tony and Jenkinson, Brian},
  year={2007},
  publisher={Springer}
}

@inproceedings{pollitt_forensics_history_2010,
  title={A history of digital forensics},
  author={Pollitt, Mark},
  booktitle={IFIP International Conference on Digital Forensics},
  pages={3--15},
  year={2010},
  organization={Springer}
}

@article{rizvi_forensics_network_2022,
  title={Application of artificial intelligence to network forensics: Survey, challenges and future directions},
  author={Rizvi, Syed and Scanlon, Mark and McGibney, Jimmy and Sheppard, John},
  journal={Ieee Access},
  volume={10},
  pages={110362--110384},
  year={2022},
  publisher={IEEE}
}

@article{barmpatsalou_forensics_mobile_2013,
  title={A critical review of 7 years of Mobile Device Forensics},
  author={Barmpatsalou, Konstantia and Damopoulos, Dimitrios and Kambourakis, Georgios and Katos, Vasilios},
  journal={Digital Investigation},
  volume={10},
  number={4},
  pages={323--349},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{demontis_adversarial_transferability_2019,
  title={Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks},
  author={Demontis, Ambra and Melis, Marco and Pintor, Maura and Jagielski, Matthew and Biggio, Battista and Oprea, Alina and Nita-Rotaru, Cristina and Roli, Fabio},
  booktitle={28th USENIX security symposium (USENIX security 19)},
  pages={321--338},
  year={2019}
}

@inproceedings{Chen_zoo_black_box_dnn_2017,
  author={Chen, Tianqi and Goodfellow, Ian J. and Papernot, Nicolas and Shlens, Jonathon and Ustyuzhanin, Anton},
  title={ZOO: Zeroth Order Optimization Based Black-Box Attacks to Deep Neural Networks without Training Surrogate Models},
  booktitle={Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
  year={2017},
  pages={15--26}
}

@incollection{kurakin_adversarial_2018,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
  booktitle={Artificial intelligence safety and security},
  pages={99--112},
  year={2018},
  publisher={Chapman and Hall/CRC}
}

@article{madry_dnn_adv_resistant_2017,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@inproceedings{carlini_nn_robustness_eval_2017,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={{2017 IEEE Symposium on Security and Privacy (SP)}},
  pages={39--57},
  year={2017},
  organization={Ieee}
}

@article{badjie_image_classification_attacks_review_2024,
author = {Badjie, Bakary and Cec\'{\i}lio, Jos\'{e} and Casimiro, Antonio},
title = {Adversarial Attacks and Countermeasures on Image Classification-based Deep Learning Models in Autonomous Driving Systems: A Systematic Review},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3691625},
doi = {10.1145/3691625},
abstract = {The rapid development of artificial intelligence (AI) and breakthroughs in Internet of Things (IoT) technologies have driven the innovation of advanced autonomous driving systems (ADSs). Image classification deep learning (DL) algorithms immensely contribute to the decision-making process in ADSs, showcasing their capabilities in handling complex real-world driving scenarios, surpassing human driving intelligence. However, these algorithms are vulnerable to adversarial attacks, which aim to fool them in real-time decision-making and compromise the reliability of the autonomous driving functions. This systematic review offers a comprehensive overview of the most recent literature on adversarial attacks and countermeasures on image classification DL models in ADSs. The review highlights the current challenges in applying successful countermeasures to mitigating these vulnerabilities. We also introduce taxonomies for categorizing adversarial attacks and countermeasures and provide recommendations and guidelines to help researchers design and evaluate countermeasures. We suggest interesting future research directions to improve the robustness of image classification DL models against adversarial attacks in autonomous driving scenarios.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {20},
numpages = {52},
keywords = {Adversarial attacks, defense and countermeasures, autonomous driving systems, deep learning, image classification}
}

@inproceedings{zhang_inversion_2020,
  title={The secret revealer: Generative model-inversion attacks against deep neural networks},
  author={Zhang, Yuheng and Jia, Ruoxi and Pei, Hengzhi and Wang, Wenxiao and Li, Bo and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={253--261},
  year={2020}
}

@article{yang_inversion_survey_2025,
  title={Deep learning model inversion attacks and defenses: a comprehensive survey},
  author={Yang, Wencheng and Wang, Song and Wu, Di and Cai, Taotao and Zhu, Yanming and Wei, Shicheng and Zhang, Yiying and Yang, Xu and Tang, Zhaohui and Li, Yan},
  journal={Artificial Intelligence Review},
  volume={58},
  number={8},
  pages={242},
  year={2025},
  publisher={Springer}
}
@inproceedings{nasr_deep_learning_privacy_2018,
  title={Comprehensive privacy analysis of deep learning},
  author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  booktitle={Proceedings of the 2019 IEEE Symposium on Security and Privacy (SP)},
  volume={2018},
  pages={1--15},
  year={2018}
}
@misc{6_ai_attacks,
    author={Mindgard},      
    title={AI Under Attack: Six Key Adversarial Attacks and Their Consequences}, 
    year={2025},
    url={https://mindgard.ai/blog/ai-under-attack-six-key-adversarial-attacks-and-their-consequences}
}
@inproceedings{guo2020_poisoning_nn,
  title={Practical poisoning attacks on neural networks},
  author={Guo, Junfeng and Liu, Cong},
  booktitle={European Conference on Computer Vision},
  pages={142--158},
  year={2020},
  organization={Springer}
}
@ARTICLE{yudong_2023_backdoor_dnn,
  author={Li, Yudong and Zhang, Shigeng and Wang, Weiping and Song, Hong},
  journal={IEEE Open Journal of the Computer Society}, 
  title={Backdoor Attacks to Deep Learning Models and Countermeasures: A Survey}, 
  year={2023},
  volume={4},
  number={},
  pages={134-146},
  keywords={Deep learning;Face recognition;Data models;Computational modeling;Training;Perturbation methods;Video on demand;Deep learning;security;backdoor attack},
  doi={10.1109/OJCS.2023.3267221}
}
@article{shafahi2018_poison_clean_label,
  title={Poison frogs! targeted clean-label poisoning attacks on neural networks},
  author={Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{baggili2019founding_ai_forensics,
  title={Founding the domain of AI forensics},
  author={Baggili, Ibrahim and Behzadan, Vahid},
  journal={arXiv preprint arXiv:1912.06497},
  year={2019}
}
@inproceedings{edwards2021_ai_forensics,
  title={On exploring the sub-domain of artificial intelligence (ai) model forensics},
  author={Edwards, Tiffanie and McCullough, Syria and Nassar, Mohamed and Baggili, Ibrahim},
  booktitle={International Conference on Digital Forensics and Cyber Crime},
  pages={35--51},
  year={2021},
  organization={Springer}
}
@incollection{losavio2023ai_forensic,
  title={Forensic proof and criminal liability for development, distribution and use of artificial intelligence},
  author={Losavio, Michael},
  booktitle={AI embedded assurance for cyber systems},
  pages={37--48},
  year={2023},
  publisher={Springer}
}
@article{schneider2023towards_ai_forensics,
  title={Towards AI forensics: Did the artificial intelligence system do it?},
  author={Schneider, Johannes and Breitinger, Frank},
  journal={Journal of Information Security and Applications},
  volume={76},
  pages={103517},
  year={2023},
  publisher={Elsevier}
}
@article{manasa2022forensics_attack_on_ai,
  title={Digital forensics investigation for attacks on artificial intelligence},
  author={Manasa, Sanyasi and Kumar, Kukatlapalli Pradeep},
  journal={ECS Transactions},
  volume={107},
  number={1},
  pages={19639},
  year={2022},
  publisher={IOP Publishing}
}
@article{jeong2020ai_security_and_forensics,
  title={Artificial intelligence security threat, crime, and forensics: Taxonomy and open issues},
  author={Jeong, Doowon},
  journal={IEEE Access},
  volume={8},
  pages={184560--184574},
  year={2020},
  publisher={IEEE}
}

@article{JIN2022_Decision_boundary,
title = {ROBY: Evaluating the adversarial robustness of a deep model by its decision boundaries},
journal = {Information Sciences},
volume = {587},
pages = {97-122},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521012421},
author = {Haibo Jin and Jinyin Chen and Haibin Zheng and Zhen Wang and Jun Xiao and Shanqing Yu and Zhaoyan Ming},
keywords = {Robustness evaluation, Deep learning, Deep neural network, Decision boundaries},
abstract = {With the successful applications of DNNs in many real-world tasks, model’s robustness has raised public concern. Recently the robustness of deep models is often evaluated by purposely generated adversarial samples, which is time-consuming and usually dependent on the specific attacks and model structures. Addressing the problem, we propose a generic evaluation metric ROBY, a novel attack-independent robustness measurement based on the model’s feature distribution. Without prior knowledge of adversarial samples, ROBY uses inter-class and intra-class statistics to capture the features in the latent space. Models with stronger robustness always have larger distances between classes and smaller distances in the same class. Comprehensive experiments have been conducted on ten state-of-the-art deep models and different datasets to verify ROBY’s effectiveness and efficiency. Compared with other evaluation metrics, ROBY better matches the robustness golden standard attack success rate (ASR), with significantly less computation cost. To the best of our knowledge, ROBY is the first light-weighted attack-independent robustness evaluation metric general to a wide range of deep models. The code of it can be downloaded at https://github.com/Allen-piexl/ROBY.}
}

@inproceedings{li2021_nonlinear_gradient_estimation,
  title={Nonlinear projection based gradient estimation for query efficient blackbox attacks},
  author={Li, Huichen and Li, Linyi and Xu, Xiaojun and Zhang, Xiaolu and Yang, Shuang and Li, Bo},
  booktitle={International conference on artificial intelligence and statistics},
  pages={3142--3150},
  year={2021},
  organization={PMLR}
}

@article{tsai2025beyond_nlp_code_poisoning,
  title={Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in Code Generation Datasets},
  author={Tsai, Chi-Chien and Yu, Chia-Mu and Lin, Ying-Dar and Wu, Yu-Sung and Lee, Wei-Bin},
  journal={arXiv preprint arXiv:2502.20246},
  year={2025}
}

@article{lyon2023_chatgpt_fake_legal,
  title={Fake cases, real consequences: misuse of ChatGPT leads to sanctions},
  author={Lyon, Christopher F},
  journal={NY Litigator},
  volume={28},
  number={2},
  pages={8--12},
  year={2023}
}

@inproceedings{ji2023_mitigate_llm_hallucination,
  title={Towards mitigating LLM hallucination via self reflection},
  author={Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={1827--1843},
  year={2023}
}

@inproceedings{kim2025_llm_overliance,
  title={Fostering appropriate reliance on large language models: The role of explanations, sources, and inconsistencies},
  author={Kim, Sunnie SY and Vaughan, Jennifer Wortman and Liao, Q Vera and Lombrozo, Tania and Russakovsky, Olga},
  booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2025}
}

@misc{sans_ir_2026,
      title={Incident Response}, 
      author={SANS},
      year={2026},
      url={https://www.sans.org/security-resources/glossary-of-terms/incident-response}, 
}

@misc{tay_chatbot_2016,
      title={Tay (chatbot)}, 
      author={Wikipedia},
      year={2016},
      url={https://en.wikipedia.org/wiki/Tay_(chatbot)}, 
}

@article{rice_2019_driverless_tesla_challenges,
  title={The driverless car and the legal system: Hopes and fears as the courts, regulatory agencies, waymo, tesla, and uber deal with this exciting and terrifying new technology},
  author={Rice, Daniel},
  journal={Journal of Strategic Innovation and Sustainability},
  volume={14},
  number={1},
  pages={134--146},
  year={2019},
  publisher={North American Business Press}
}

@misc{deepfake_fraud_2024,
      title={A Deepfake Scammed a Bank out of $25M — Now What?}, 
      author={Trend Micro - United Kingdom (GB)},
      year={2024},
      url={https://www.trendmicro.com/en_gb/research/24/b/deepfake-video-calls.html}, 
}

@article{nist_ai_rmf_2023,
  title={Artificial intelligence risk management framework (AI RMF 1.0)},
  author={AI, NIST},
  journal={URL: https://nvlpubs. nist. gov/nistpubs/ai/nist. ai},
  pages={100--1},
  year={2023}
}

@misc{eu_trustworthy_ai_2024,
      title={Ethics guidelines for trustworthy AI}, 
      author={European Commission - High Level Expert Group on Artificial Intelligence},
      year={2019},
      url={https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai}, 
}
